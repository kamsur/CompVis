{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Infant Vision Acuity Simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, Subset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import ImageFolder\n",
    "from PIL import Image, ImageFilter\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Custom Infant Vision Dataset Class\n",
    "class InfantVision(Dataset):\n",
    "    def __init__(self, imagenet_subset, ages_in_months, transform=None, use_transform=True):  # use_transform is a flag to see if blurs are neededd\n",
    "        self.imagenet_subset = imagenet_subset # A subset of the dataset (in this case 100 random images)\n",
    "        self.ages_in_months = ages_in_months\n",
    "        self.transform = transform\n",
    "        self.use_transform = use_transform\n",
    "\n",
    "    def _age_to_blur(self, age_in_months):\n",
    "        if age_in_months <= 2:\n",
    "            return 4  # Maximum blur for newborns (0-2 months)\n",
    "        elif age_in_months <= 4:\n",
    "            return 3  # Blur for infants aged 2-4 months\n",
    "        elif age_in_months <= 6:\n",
    "            return 2  # Blur for infants aged 4-6 months\n",
    "        elif age_in_months <= 9:\n",
    "            return 1  # Blur for infants aged 6-9 months\n",
    "        else:\n",
    "            return 0  # No blur for infants older than 9 months\n",
    "\n",
    "    def _apply_gaussian_blur(self, image, sigma):\n",
    "        if self.use_transform and sigma > 0:\n",
    "            return image.filter(ImageFilter.GaussianBlur(radius=sigma))\n",
    "        else:\n",
    "            return image  # No blur applied\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imagenet_subset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image, _ = self.imagenet_subset[idx]\n",
    "        age_in_months = self.ages_in_months[idx]\n",
    "        sigma = self._age_to_blur(age_in_months)\n",
    "        \n",
    "        # Apply Gaussian blur based on age\n",
    "        image = self._apply_gaussian_blur(image, sigma)\n",
    "        \n",
    "        # Apply transformations, including resizing\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        \n",
    "        return image, age_in_months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-100 and select a subset of images\n",
    "transform = transforms.Compose([\n",
    "    # transforms.Resize((112, 112)),  # Resize images to 112x112 to match the paper\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "])\n",
    "\n",
    "# Set path to your ImageNet Mini dataset\n",
    "imagenet_path = \"D:/Uni/WS24-25/Computational Visual Preception/imagenet-mini/archive/imagenet-mini\"  # path to mini imagenet\n",
    "imagenet_data = ImageFolder(root=imagenet_path)\n",
    "\n",
    "# Select a random subset of 100 images\n",
    "subset_indices = random.sample(range(len(imagenet_data)), 100)\n",
    "imagenet_subset = torch.utils.data.Subset(imagenet_data, subset_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give ages in months for each image, randomly\n",
    "ages_in_months = [random.choice([0, 2, 4, 6, 9, 12]) for _ in range(len(imagenet_subset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Infant Vision Dataset with CIFAR-100 Subset\n",
    "infant_vision_dataset = InfantVision(imagenet_subset, ages_in_months, transform=transform)\n",
    "dataloader = DataLoader(infant_vision_dataset, batch_size=1, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Tensors to PIL Images (for Visualization)\n",
    "def tensor_to_pil(image_tensor):\n",
    "    image_tensor = image_tensor.squeeze()  # remove batch dimension\n",
    "    image = transforms.ToPILImage()(image_tensor)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'InfantVision' object has no attribute 'use_transformtransform'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Display Original and Blurred Examples for Each Sigma Level\u001b[39;00m\n\u001b[0;32m      2\u001b[0m displayed_sigmas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[1;32m----> 4\u001b[0m \u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mage\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minfant_vision_dataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_age_to_blur\u001b[49m\u001b[43m(\u001b[49m\u001b[43mage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Only display if this sigma level hasn't been shown yet\u001b[39;49;00m\n",
      "File \u001b[1;32md:\\Uni\\WS24-25\\Computational Visual Preception\\first code submission\\computational-visual-preception\\infantVision\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32md:\\Uni\\WS24-25\\Computational Visual Preception\\first code submission\\computational-visual-preception\\infantVision\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32md:\\Uni\\WS24-25\\Computational Visual Preception\\first code submission\\computational-visual-preception\\infantVision\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[1;32mIn[5], line 36\u001b[0m, in \u001b[0;36mInfantVision.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m     33\u001b[0m sigma \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_age_to_blur(age_in_months)\n\u001b[0;32m     35\u001b[0m \u001b[38;5;66;03m# Apply Gaussian blur based on age\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply_gaussian_blur\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Apply transformations, including resizing\u001b[39;00m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform:\n",
      "Cell \u001b[1;32mIn[5], line 22\u001b[0m, in \u001b[0;36mInfantVision._apply_gaussian_blur\u001b[1;34m(self, image, sigma)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply_gaussian_blur\u001b[39m(\u001b[38;5;28mself\u001b[39m, image, sigma):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_transformtransform\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m sigma \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     23\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m image\u001b[38;5;241m.\u001b[39mfilter(ImageFilter\u001b[38;5;241m.\u001b[39mGaussianBlur(radius\u001b[38;5;241m=\u001b[39msigma))\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'InfantVision' object has no attribute 'use_transformtransform'"
     ]
    }
   ],
   "source": [
    "# Display Original and Blurred Examples for Each Sigma Level\n",
    "displayed_sigmas = set()\n",
    "\n",
    "for i, (image, age) in enumerate(dataloader):\n",
    "    sigma = infant_vision_dataset._age_to_blur(age.item())\n",
    "    \n",
    "    # Only display if this sigma level hasn't been shown yet\n",
    "    if sigma not in displayed_sigmas:\n",
    "        # Retrieve the original, unaltered image\n",
    "        original_image, _ = imagenet_subset[i]\n",
    "        \n",
    "        # Convert both original and blurred images to displayable format\n",
    "        original_pil_image = transforms.ToPILImage()(transforms.ToTensor()(original_image))\n",
    "        blurred_pil_image = transforms.ToPILImage()(image.squeeze(0))\n",
    "        \n",
    "        # Display side by side for comparison\n",
    "        fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axs[0].imshow(original_pil_image)\n",
    "        axs[0].set_title(\"Original Image\")\n",
    "        axs[0].axis('off')\n",
    "        \n",
    "        axs[1].imshow(blurred_pil_image)\n",
    "        axs[1].set_title(f\"Blurred Image\\nSigma: {sigma} (Age: {age.item()} months)\")\n",
    "        axs[1].axis('off')\n",
    "        \n",
    "        plt.show()\n",
    "        \n",
    "        # Mark this sigma level as displayed\n",
    "        displayed_sigmas.add(sigma)\n",
    "    \n",
    "    # Stop once we've displayed all sigma levels\n",
    "    if len(displayed_sigmas) == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evalutation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "# Function to measure total data loading time\n",
    "def measure_total_loading_time(dataloader):\n",
    "    start_time = time.time()\n",
    "    for _ in dataloader:\n",
    "        pass  # Simulate the loading of data\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "infant_vision_dataset_without_transform = InfantVision(\n",
    "    imagenet_subset,  # Subset of 100 random images\n",
    "    ages_in_months,   # Corresponding ages in months\n",
    "    transform=transform,    # Transform to tensor\n",
    "    use_transform=False # To surprass the use of the transforms (gaussian blurs)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken with transformation: 0.85 seconds\n",
      "Time taken without transformation: 0.84 seconds\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Measure loading time with transformation\n",
    "dataloader_with_transform = DataLoader(infant_vision_dataset, batch_size=1, shuffle=False)\n",
    "time_with_transform = measure_total_loading_time(dataloader_with_transform)\n",
    "\n",
    "# Measure loading time without transformation\n",
    "dataloader_without_transform = DataLoader(infant_vision_dataset_without_transform, batch_size=1, shuffle=False)\n",
    "time_without_transform = measure_total_loading_time(dataloader_without_transform)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Time taken with transformation: {time_with_transform:.2f} seconds\")\n",
    "print(f\"Time taken without transformation: {time_without_transform:.2f} seconds\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (infantVision)",
   "language": "python",
   "name": "infantvision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
